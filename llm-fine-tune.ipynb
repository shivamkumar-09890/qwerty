{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8655317,"sourceType":"datasetVersion","datasetId":5185051}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict, Dataset\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoConfig, \n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding,\n    TrainingArguments,\n    Trainer)\n\nfrom peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\nimport evaluate\nimport torch\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:09:32.016219Z","iopub.execute_input":"2024-06-10T22:09:32.016697Z","iopub.status.idle":"2024-06-10T22:09:33.331080Z","shell.execute_reply.started":"2024-06-10T22:09:32.016658Z","shell.execute_reply":"2024-06-10T22:09:33.330323Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!pip install peft\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:09:16.127346Z","iopub.execute_input":"2024-06-10T22:09:16.128109Z","iopub.status.idle":"2024-06-10T22:09:28.880919Z","shell.execute_reply.started":"2024-06-10T22:09:16.128071Z","shell.execute_reply":"2024-06-10T22:09:28.879802Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.41.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.30.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.11.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install evaluate\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:08:58.315284Z","iopub.execute_input":"2024-06-10T22:08:58.315967Z","iopub.status.idle":"2024-06-10T22:09:11.925083Z","shell.execute_reply.started":"2024-06-10T22:08:58.315938Z","shell.execute_reply":"2024-06-10T22:09:11.923946Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.19.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:09:43.779424Z","iopub.execute_input":"2024-06-10T22:09:43.779759Z","iopub.status.idle":"2024-06-10T22:09:43.797341Z","shell.execute_reply.started":"2024-06-10T22:09:43.779735Z","shell.execute_reply":"2024-06-10T22:09:43.796581Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/mental-health/mental_health.csv')\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:09:47.171429Z","iopub.execute_input":"2024-06-10T22:09:47.171790Z","iopub.status.idle":"2024-06-10T22:09:47.522549Z","shell.execute_reply.started":"2024-06-10T22:09:47.171760Z","shell.execute_reply":"2024-06-10T22:09:47.521523Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                text  label\n0  dear american teens question dutch person hear...      0\n1  nothing look forward lifei dont many reasons k...      1\n2  music recommendations im looking expand playli...      0\n3  im done trying feel betterthe reason im still ...      1\n4  worried  year old girl subject domestic physic...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dear american teens question dutch person hear...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>nothing look forward lifei dont many reasons k...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>music recommendations im looking expand playli...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>im done trying feel betterthe reason im still ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>worried  year old girl subject domestic physic...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"label = dataset['label']\ntext = dataset['text']","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:09:53.168565Z","iopub.execute_input":"2024-06-10T22:09:53.169286Z","iopub.status.idle":"2024-06-10T22:09:53.176605Z","shell.execute_reply.started":"2024-06-10T22:09:53.169233Z","shell.execute_reply":"2024-06-10T22:09:53.175256Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Split the data into 70% train and 30% validation/test\ntrain_texts, val_test_texts, train_labels, val_test_labels = train_test_split(text, label, test_size=0.3, random_state=42)\n\n# Further split the 30% into 50% validation and 50% test\nval_texts, test_texts, val_labels, test_labels = train_test_split(val_test_texts, val_test_labels, test_size=0.5, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:09:55.607407Z","iopub.execute_input":"2024-06-10T22:09:55.608304Z","iopub.status.idle":"2024-06-10T22:09:55.623859Z","shell.execute_reply.started":"2024-06-10T22:09:55.608266Z","shell.execute_reply":"2024-06-10T22:09:55.623038Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Create Dataset objects\ntrain_dataset = Dataset.from_dict({'label': train_labels, 'text': train_texts})\nval_dataset = Dataset.from_dict({'label': val_labels, 'text': val_texts})\ntest_dataset = Dataset.from_dict({'label': test_labels, 'text': test_texts})","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:09:58.079395Z","iopub.execute_input":"2024-06-10T22:09:58.080094Z","iopub.status.idle":"2024-06-10T22:09:58.219674Z","shell.execute_reply.started":"2024-06-10T22:09:58.080062Z","shell.execute_reply":"2024-06-10T22:09:58.218917Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\n\n# Create DatasetDict containing training, validation, and testing datasets\ndataset = DatasetDict({'train': train_dataset, 'validation': val_dataset, 'test': test_dataset})\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:10:00.695679Z","iopub.execute_input":"2024-06-10T22:10:00.696054Z","iopub.status.idle":"2024-06-10T22:10:00.700439Z","shell.execute_reply.started":"2024-06-10T22:10:00.696022Z","shell.execute_reply":"2024-06-10T22:10:00.699549Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:10:03.835281Z","iopub.execute_input":"2024-06-10T22:10:03.836144Z","iopub.status.idle":"2024-06-10T22:10:03.842231Z","shell.execute_reply.started":"2024-06-10T22:10:03.836111Z","shell.execute_reply":"2024-06-10T22:10:03.841334Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['label', 'text'],\n        num_rows: 19583\n    })\n    validation: Dataset({\n        features: ['label', 'text'],\n        num_rows: 4197\n    })\n    test: Dataset({\n        features: ['label', 'text'],\n        num_rows: 4197\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# display % of training data with label=1\nnp.array(dataset['train']['label']).sum()/len(dataset['train']['label'])","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:10:07.951393Z","iopub.execute_input":"2024-06-10T22:10:07.952409Z","iopub.status.idle":"2024-06-10T22:10:07.983702Z","shell.execute_reply.started":"2024-06-10T22:10:07.952375Z","shell.execute_reply":"2024-06-10T22:10:07.982791Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"0.4967573916151764"},"metadata":{}}]},{"cell_type":"code","source":"model_checkpoint = 'distilbert-base-uncased'\n# model_checkpoint = 'roberta-base' # you can alternatively use roberta-base but this model is bigger thus training will take longer\n\n# define label maps\nid2label = {0: \"Negative\", 1: \"Positive\"}\nlabel2id = {\"Negative\":0, \"Positive\":1}\n\n# generate classification model from model_checkpoint\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:10:10.779535Z","iopub.execute_input":"2024-06-10T22:10:10.780274Z","iopub.status.idle":"2024-06-10T22:10:12.830407Z","shell.execute_reply.started":"2024-06-10T22:10:10.780244Z","shell.execute_reply":"2024-06-10T22:10:12.829484Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0144caec0914319872719dd63c5fde4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fae934cc52194fbfa45620610c515ec6"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# display architecture\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:10:19.074617Z","iopub.execute_input":"2024-06-10T22:10:19.075476Z","iopub.status.idle":"2024-06-10T22:10:19.083232Z","shell.execute_reply.started":"2024-06-10T22:10:19.075435Z","shell.execute_reply":"2024-06-10T22:10:19.082242Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# create tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n\n# add pad token if none exists\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    model.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:10:27.595418Z","iopub.execute_input":"2024-06-10T22:10:27.595781Z","iopub.status.idle":"2024-06-10T22:10:28.656667Z","shell.execute_reply.started":"2024-06-10T22:10:27.595754Z","shell.execute_reply":"2024-06-10T22:10:28.655700Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5752c12b6c2b4dbc8633314f055c0dc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71b369a29171468fb7d28b5bef271a18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5799fdef9f04a11a7dfb4342faa0feb"}},"metadata":{}}]},{"cell_type":"code","source":"# create tokenize function\ndef tokenize_function(examples):\n    # extract text\n    text = examples[\"text\"]\n\n    #tokenize and truncate text\n    tokenizer.truncation_side = \"left\"\n    tokenized_inputs = tokenizer(\n        text,\n        return_tensors=\"np\",\n        truncation=True,\n        max_length=512\n    )\n\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:10:34.935476Z","iopub.execute_input":"2024-06-10T22:10:34.936320Z","iopub.status.idle":"2024-06-10T22:10:34.941066Z","shell.execute_reply.started":"2024-06-10T22:10:34.936288Z","shell.execute_reply":"2024-06-10T22:10:34.940121Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# tokenize training and validation datasets\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\ntokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:10:41.071750Z","iopub.execute_input":"2024-06-10T22:10:41.072566Z","iopub.status.idle":"2024-06-10T22:10:45.830788Z","shell.execute_reply.started":"2024-06-10T22:10:41.072536Z","shell.execute_reply":"2024-06-10T22:10:45.829917Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19583 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ed247216ed640429672b06282698a1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4197 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce460596e04a468c829afb8820332f06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4197 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe65f951dd264279a9d335e6b79f64c7"}},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['label', 'text', 'input_ids', 'attention_mask'],\n        num_rows: 19583\n    })\n    validation: Dataset({\n        features: ['label', 'text', 'input_ids', 'attention_mask'],\n        num_rows: 4197\n    })\n    test: Dataset({\n        features: ['label', 'text', 'input_ids', 'attention_mask'],\n        num_rows: 4197\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# create data collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:10:50.875639Z","iopub.execute_input":"2024-06-10T22:10:50.876331Z","iopub.status.idle":"2024-06-10T22:10:50.880737Z","shell.execute_reply.started":"2024-06-10T22:10:50.876296Z","shell.execute_reply":"2024-06-10T22:10:50.879629Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# import accuracy evaluation metric\naccuracy = evaluate.load(\"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:10:53.391209Z","iopub.execute_input":"2024-06-10T22:10:53.391567Z","iopub.status.idle":"2024-06-10T22:10:53.939895Z","shell.execute_reply.started":"2024-06-10T22:10:53.391539Z","shell.execute_reply":"2024-06-10T22:10:53.939037Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09acbcbc45f847d8ba083a62d5433e6a"}},"metadata":{}}]},{"cell_type":"code","source":"# define an evaluation function to pass into trainer later\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=1)\n\n    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:10:56.931541Z","iopub.execute_input":"2024-06-10T22:10:56.932257Z","iopub.status.idle":"2024-06-10T22:10:56.937125Z","shell.execute_reply.started":"2024-06-10T22:10:56.932227Z","shell.execute_reply":"2024-06-10T22:10:56.936125Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# define list of examples\ntext_list = [\"I am not happy\", \"life is so borring.\", \"today was the best day of my life i really loved it\", \"i passed my exam. I am very happy\", \"This life is great\"]\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:48:53.065929Z","iopub.execute_input":"2024-06-10T22:48:53.066841Z","iopub.status.idle":"2024-06-10T22:48:53.072932Z","shell.execute_reply.started":"2024-06-10T22:48:53.066809Z","shell.execute_reply":"2024-06-10T22:48:53.071843Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# define list of examples\ntext_list = [\"I am so depressed.\", \"life is so borring.\", \"today was the best day of my life i really loved it\", \"i passed my exam. I am very happy\", \"This life is great\"]\n\nprint(\"Untrained model predictions:\")\nprint(\"----------------------------\")\nfor text in text_list:\n    # tokenize text\n    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n    # compute logits\n    logits = model(inputs).logits\n    # convert logits to label\n    predictions = torch.argmax(logits)\n\n    print(text + \" - \" + id2label[predictions.tolist()])","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:11:00.351477Z","iopub.execute_input":"2024-06-10T22:11:00.351830Z","iopub.status.idle":"2024-06-10T22:11:00.635962Z","shell.execute_reply.started":"2024-06-10T22:11:00.351802Z","shell.execute_reply":"2024-06-10T22:11:00.635033Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Untrained model predictions:\n----------------------------\nI am so depressed. - Positive\nlife is so borring. - Positive\ntoday was the best day of my life i really loved it - Positive\ni passed my exam. I am very happy - Positive\nThis life is great - Positive\n","output_type":"stream"}]},{"cell_type":"code","source":"peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n                        r=4,\n                        lora_alpha=32,\n                        lora_dropout=0.01,\n                        target_modules = ['q_lin'])","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:11:06.427685Z","iopub.execute_input":"2024-06-10T22:11:06.428053Z","iopub.status.idle":"2024-06-10T22:11:06.432788Z","shell.execute_reply.started":"2024-06-10T22:11:06.428025Z","shell.execute_reply":"2024-06-10T22:11:06.431802Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"peft_config","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:11:12.147656Z","iopub.execute_input":"2024-06-10T22:11:12.148490Z","iopub.status.idle":"2024-06-10T22:11:12.154414Z","shell.execute_reply.started":"2024-06-10T22:11:12.148448Z","shell.execute_reply":"2024-06-10T22:11:12.153283Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=4, target_modules={'q_lin'}, lora_alpha=32, lora_dropout=0.01, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"},"metadata":{}}]},{"cell_type":"code","source":"model = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:11:15.547286Z","iopub.execute_input":"2024-06-10T22:11:15.547649Z","iopub.status.idle":"2024-06-10T22:11:15.570170Z","shell.execute_reply.started":"2024-06-10T22:11:15.547620Z","shell.execute_reply":"2024-06-10T22:11:15.569232Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"trainable params: 628,994 || all params: 67,584,004 || trainable%: 0.9307\n","output_type":"stream"}]},{"cell_type":"code","source":"# hyperparameters\nlr = 1e-3\nbatch_size = 16\nnum_epochs = 10","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:15:04.315972Z","iopub.execute_input":"2024-06-10T22:15:04.316895Z","iopub.status.idle":"2024-06-10T22:15:04.321023Z","shell.execute_reply.started":"2024-06-10T22:15:04.316862Z","shell.execute_reply":"2024-06-10T22:15:04.320047Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# define training arguments\ntraining_args = TrainingArguments(\n    output_dir= model_checkpoint + \"-lora-text-classification\",\n    learning_rate=lr,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:15:07.763751Z","iopub.execute_input":"2024-06-10T22:15:07.764152Z","iopub.status.idle":"2024-06-10T22:15:07.819484Z","shell.execute_reply.started":"2024-06-10T22:15:07.764120Z","shell.execute_reply":"2024-06-10T22:15:07.818554Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# creater trainer object\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n    compute_metrics=compute_metrics,\n)\n\n# train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:15:25.627396Z","iopub.execute_input":"2024-06-10T22:15:25.628166Z","iopub.status.idle":"2024-06-10T22:34:40.428038Z","shell.execute_reply.started":"2024-06-10T22:15:25.628133Z","shell.execute_reply":"2024-06-10T22:34:40.426432Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240610_221541-3qhp0e7s</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/shivam-kumar878/huggingface/runs/3qhp0e7s' target=\"_blank\">distilbert-base-uncased-lora-text-classification</a></strong> to <a href='https://wandb.ai/shivam-kumar878/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/shivam-kumar878/huggingface' target=\"_blank\">https://wandb.ai/shivam-kumar878/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/shivam-kumar878/huggingface/runs/3qhp0e7s' target=\"_blank\">https://wandb.ai/shivam-kumar878/huggingface/runs/3qhp0e7s</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4585' max='12240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 4585/12240 18:39 < 31:09, 4.09 it/s, Epoch 3.75/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.190500</td>\n      <td>0.143677</td>\n      <td>{'accuracy': 0.9456754824874911}</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.142300</td>\n      <td>0.168942</td>\n      <td>{'accuracy': 0.9449606862044317}</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.137300</td>\n      <td>0.175998</td>\n      <td>{'accuracy': 0.9459137479151775}</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"{'accuracy': 0.9456754824874911}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'accuracy': 0.9449606862044317}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'accuracy': 0.9459137479151775}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 13\u001b[0m\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      4\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2221\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"trained_model = trainer.model","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:47:53.692704Z","iopub.execute_input":"2024-06-10T22:47:53.693097Z","iopub.status.idle":"2024-06-10T22:47:53.699197Z","shell.execute_reply.started":"2024-06-10T22:47:53.693062Z","shell.execute_reply":"2024-06-10T22:47:53.698144Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"print(trained_model)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:36:22.697203Z","iopub.execute_input":"2024-06-10T22:36:22.697993Z","iopub.status.idle":"2024-06-10T22:36:22.706260Z","shell.execute_reply.started":"2024-06-10T22:36:22.697950Z","shell.execute_reply":"2024-06-10T22:36:22.705228Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): DistilBertForSequenceClassification(\n      (distilbert): DistilBertModel(\n        (embeddings): Embeddings(\n          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n          (position_embeddings): Embedding(512, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (transformer): Transformer(\n          (layer): ModuleList(\n            (0-5): 6 x TransformerBlock(\n              (attention): MultiHeadSelfAttention(\n                (dropout): Dropout(p=0.1, inplace=False)\n                (q_lin): lora.Linear(\n                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.01, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=4, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=4, out_features=768, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n                (v_lin): Linear(in_features=768, out_features=768, bias=True)\n                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n              )\n              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (ffn): FFN(\n                (dropout): Dropout(p=0.1, inplace=False)\n                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n                (activation): GELUActivation()\n              )\n              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            )\n          )\n        )\n      )\n      (pre_classifier): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=768, out_features=768, bias=True)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (classifier): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=768, out_features=2, bias=True)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=768, out_features=2, bias=True)\n        )\n      )\n      (dropout): Dropout(p=0.2, inplace=False)\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\ntrained_model.to('cpu')  # Move trained model to CPU\n\nprint(\"Trained model predictions:\")\nprint(\"--------------------------\")\nfor text in text_list:\n    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n\n    logits = trained_model(inputs).logits\n    predictions = torch.max(logits, 1).indices\n\n    print(text + \" - \" + id2label[predictions.tolist()[0]])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T22:49:01.045618Z","iopub.execute_input":"2024-06-10T22:49:01.046724Z","iopub.status.idle":"2024-06-10T22:49:01.261128Z","shell.execute_reply.started":"2024-06-10T22:49:01.046679Z","shell.execute_reply":"2024-06-10T22:49:01.260091Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Trained model predictions:\n--------------------------\nI am not happy - Negative\nlife is so borring. - Negative\ntoday was the best day of my life i really loved it - Positive\ni passed my exam. I am very happy - Negative\nThis life is great - Negative\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}